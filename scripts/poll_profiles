#!/usr/bin/env python
#
# Usage: python poll_profiles.py <profiles-glob> <enrichment-service-URI>

import sys, os, glob, fnmatch
import argparse
import base64
import datetime, time
from itertools import groupby
from urllib import urlencode
from amara.thirdparty import json, httplib2
from amara.lib.iri import is_absolute, join
from amara import bindery
import xmltodict
from math import floor

# FIXME Turns out this isn't always correct. Sometimes Series files are located in
# different directories than its Item files. No clear deterministic path between
# them, so will have to find the file instead using globbing
#ARC_RELATED_FILE = lambda srcdir, htype, hid: os.path.join(os.path.dirname(srcdir+os.sep),"%s_%s.xml"%(htype.replace(' ',''),hid))
ARC_RELATED_FILE = lambda srcdir, htype, hid: os.path.join(os.sep.join(os.path.dirname(srcdir+os.sep).split(os.sep)[:-1]),"*","%s_%s.xml"%(htype.replace(' ',''),hid))

URI_BASE = None
ENRICH = "/enrich" # enrichment service URI
H = httplib2.Http('/tmp/.pollcache')
H.force_exception_as_status_code = True

def process_profile(uri_base, profile_f):
    global URI_BASE, ENRICH

    fprof = open(profile_f,'r')
    try:
        profile = json.load(fprof)
    except Exception as e:
        profile = None

    fprof.close()

    if not profile:
        print >> sys.stderr, 'Error reading source profile.'
        print >> sys.stderr, "Detailed error information:"
        print >> sys.stderr, e
        return False

    # Pause in secs between collection ingests
    sleep = profile.get(u'sleep',0)

    URI_BASE = uri_base
    if not is_absolute(ENRICH):
        ENRICH = URI_BASE + ENRICH

    subResources = profile.get(u'subresources')
    blacklist = profile.get(u'blacklist',[])
    ptype = profile.get(u'type').lower()
    if not subResources: # i.e. all subresources
        process = TYPE_PROCESSORS.get((ptype,'all'))
        process(profile,blacklist)
    else:
        process = TYPE_PROCESSORS.get((ptype,'coll'))
        if not process:
            print >> sys.stderr, "The ingest of individual %s collections is not supported at this time"%(ptype.upper())
            sys.exit(1)

        for subr in subResources:
            process(profile, subr)
            time.sleep(sleep)

    return True

ARC_PARSE = lambda doc: xmltodict.parse(doc,xml_attribs=True,attr_prefix='',force_cdata=False,ignore_whitespace_cdata=True)

#def skip_cdata(path,key,data):
#    if '#text' in data:
#        del data['#text']
#    return key, data
#
#ARC_PARSE = lambda doc: xmltodict.parse(doc,xml_attribs=True,attr_prefix='',postprocessor=skip_cdata)
def process_arc_all(profile,blacklist=None):
    src_URL = profile.get('endpoint_URL')
    assert src_URL.startswith('file:/') # assumes no authority and the non-broken use of //
    src_dir = src_URL[5:]

    collections = {} 
    os.system("rm /tmp/coll_*")
    print "Walking directory: "+src_dir
    for (root, dirs, files) in os.walk(src_dir):
        for filename in fnmatch.filter(files, 'Item_*.xml'):
            item_fn = os.path.join(root,filename)
            item_f = open(item_fn,'r')
            item = ARC_PARSE(item_f)['archival-description']
            item_f.close()

            # set our generic identifier property
            item['_id'] = item['arc-id']

            hier_items = item['hierarchy']['hierarchy-item']
            for hi in (hier_items if isinstance(hier_items,list) else [hier_items]):
                htype = hi['hierarchy-item-lod']
                # Record Group mapped to collection
                if not htype.lower() == 'record group': continue

                hid = hi['hierarchy-item-id']

                if hid not in collections:
                    # Grab series information from item
                    coll = {}
                    coll['id'] = hid
                    coll['title'] = hi['hierarchy-item-title']
                    coll['items'] = []
                    collections[hid] = coll
                else:
                    coll = collections[hid]

                # Create tmp file to hold collections items
                coll_fn = "/tmp/coll_%s" % coll['id']
                coll_f = open(coll_fn,'a')
                coll_f.write(str(item)+"\n")
                coll_f.close()

    limit = 1000
    for cid in collections:
        # Open tmp collection file and append items
        coll_fn = "/tmp/coll_%s" % cid
        coll_f = open(coll_fn, 'r')
        lines = coll_f.readlines()
        coll_f.close()
        os.remove(coll_fn)

        i = 0
        for line in lines:
            collections[cid]['items'].append(eval(line))
            i += 1

            if i == limit or line == lines[-1]:
                print >> sys.stderr, "Enriching collection %s" % cid
                enrich_coll(profile,cid,json.dumps(collections[cid]))
                del collections[cid]['items'][:]
                i = 0   

        del collections[cid]['items']

def enrich_coll(profile,subr,content):
    # Enrich retrieved data
    global ENRICH
    
    headers = {
        "Content-Type": "application/json",
        "Pipeline-Coll": ','.join(profile["enrichments_coll"]),
        "Pipeline-Rec": ','.join(profile["enrichments_rec"]),
        "Source": profile['name'],
        "Contributor": base64.b64encode(json.dumps(profile.get(u'contributor',{})))
    }
    if subr:
        headers["Collection"] = subr

    resp, content = H.request(ENRICH,'POST',body=content,headers=headers)
    if not str(resp.status).startswith('2'):
        print >> sys.stderr, '  HTTP error with enrichment service: '+repr(resp)

def process_oai_coll(profile,subr):
    # For now, a simplifying assumption that string concatenation produces a
    # full URI from the combination of the endpoint URL and each subresource id.
    # Better might be a single field listing all URIs but unclear how that extends
    # to other protocols.

    # If multiple requests are required to harvest all information from a resource, they will
    # give us 'resumption tokens' after each request until we are done. Passing the resumption
    # token will provide the next batch of results
    global URI_BASE

    request_more, resumption_token = True, ""
    while request_more:
        endpoint = profile[u'endpoint_URL'] + (subr if subr != profile[u'name'] else "")
        if not is_absolute(endpoint):
            endpoint = URI_BASE + endpoint
        if resumption_token:
            endpoint += '&' + urlencode({'resumption_token': resumption_token})
        print >> sys.stderr, endpoint

        resp, content = H.request(endpoint)
        if not resp[u'status'].startswith('2'):
            print >> sys.stderr, '  HTTP error ('+resp[u'status']+') resolving URL: ' + endpoint
            continue
        endpoint_content = json.loads(content)
        resumption_token = endpoint_content['resumption_token']

        content = json.dumps(endpoint_content)
        enrich_coll(profile,subr,content)

        request_more = resumption_token is not None and len(resumption_token) > 0

def process_oai_all(profile,blacklist=[]):
    # Get all sets
    global URI_BASE
    url = profile[u'list_sets']
    if not is_absolute(url):
        url = URI_BASE + url
    resp, content = H.request(url)
    if not resp[u'status'].startswith('2'):
        print >> sys.stderr, ' HTTP error ('+resp[u'status']+') resolving URL: ' + url
        return False

    sleep = profile.get(u'sleep',0)
    
    subResources = []
    if len(content) > 2:
        set_content = json.loads(content)
        for s in set_content:
            if s[u'setSpec']:
                subResources.append(s[u'setSpec'])
    else:
        # Case where provider does not support Sets
        subResources.append(profile['name'])

    # Process the sets
    subr_to_process =[subr for subr in subResources if subr not in blacklist]
    for subr in subr_to_process:
        process_oai_coll(profile,subr)
        time.sleep(sleep)

def process_mets_coll(profile, subr):
    global URI_BASE
    endpoint = profile[u'endpoint_URL'].format(subr)
    resp, content = H.request(endpoint)
    if not resp[u'status'].startswith('2'):
        print >> sys.stderr, ' HTTP error (%s) resolving URL: %s' % (resp[u'status'], endpoint)
    endpoint_content = ARC_PARSE(content)
    for d in endpoint_content["mets:mets"]:
        if "mets:dmdSec" in d:
            print d[0]
    exit(0)


TYPE_PROCESSORS = {
    ('arc','coll'): None,
    ('arc','all'): process_arc_all,
    ('oai','coll'): process_oai_coll,
    ('oai','all'): process_oai_all,
    ('mets', 'coll'): process_mets_coll,
    ('mets', 'all'): None,
}

def define_arguments():
    """
    Defines command line arguments for the current script
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("uri_base", help="The base URI for the server hosting the enrichment pipeline")
    parser.add_argument("profile", help="The path to the profile(s) to be processed", nargs="+")
    return parser


def main(argv):
    parser = define_arguments()
    args = parser.parse_args(argv[1:])
    for profile in args.profile:
        print >> sys.stderr, 'Processing profile: '+profile
        process_profile(args.uri_base, profile)

if __name__ == '__main__':
    main(sys.argv)
